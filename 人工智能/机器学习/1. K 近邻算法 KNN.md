#### K 近邻算法 KNN

undefinedundefined<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

理论基础：欧式距离

**两个点在空间中的距离一般都是指欧氏距离**

- 二维平面上点a(x1,y1)与b(x2,y2)间的欧氏距离:

$$
\sqrt[2]{(x1-x2)^2+(y1-y2)^2}
$$

- 三维空间点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离:

$$
\sqrt[2]{(x1-x2)^2+(y1-y2)^2+(z1-z2)^2}
$$



##### K 近邻算法基本步骤

- 计算已知类别数据集中的点与当前之间的距离（欧式距离）
- 按照距离递增次序排序
- 选取与当前点距离最小的 K 个点
- 确定前 K 个点所在类别的出现频率
- 返回前 K 个点出现频率最高的类别作为当前的预测分类

算法实现

```python
import numpy as np


def classify(idX, dataSet, labels, k):
    lines = dataSet.shape[0]
    idXdataSet = np.tile(idX, (lines, 1))

    # 欧式距离
    sqrtDataSet = (dataSet - idXdataSet) ** 2
    # axis=1 意味着是每一行的和，而不是矩阵所有元素的和。
    sumDataSet = sqrtDataSet.sum(axis=1)
    results = sumDataSet ** 0.5

    # 返回向量从小到大的排序后的下标
    dataSetsort = results.argsort()

    classCount = dict()

    for i in range(k):
        voteLabel = labels[dataSetsort[i]]
        classCount[voteLabel] = classCount.get(voteLabel, 0) + 1

    results = sorted(classCount.items(), key=lambda x: x[1])
    return results[0][0]


dataSet = np.array([
    [1, 2, 2, 3, 4],
    [2, 3, 34, 4, 4],
    [2, 3, 4, 5, 6],
    [1, 2, 4, 2, 54]
])

labels = np.array(['A', 'B', 'C', 'D'])
inX = np.array([54, 21, 23, 5, 1])

results = classify(inX, dataSet, labels, 2)
print(results)

```

---

##### 归一化

有时候我们的样本的特征中，有某一项数值很大，比如假如有一个特征为“一天吃的米饭粒数”，这样的就基数很大，那么我们再做距离计算的时候，就会有很大的影响。所以我们需要把数值缩小，我们称为归一化。
$$
newValue = (oldValue-min)/(max-min)
$$
其中 min 和 max 分别是数据集中的最小特征值和最大特征值。虽然改变数值取值范围增加了分类器的复杂度，但为了得到准确结果，我们必须这样做。

```python
def autoNorm(dataSet):
    # 每一列的最小值
    minValue = dataSet.min(axis=0)
    # 每一列的最大值
    maxValue = dataSet.max(axis=0)
    ranges = maxValue - maxValue
    lines = dataSet.shape[0]

    temp = dataSet - np.tile(minValue, (lines, 1))

    newDataSet = temp / np.tile(ranges, (lines, 1))
    return newDataSet

```

